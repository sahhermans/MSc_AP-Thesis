{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyPfDoXB0GmT"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset,DataLoader\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eq5RvviJ0Uav"
      },
      "outputs": [],
      "source": [
        "def reset_weights(m):\n",
        "  '''\n",
        "    Try resetting model weights to avoid\n",
        "    weight leakage.\n",
        "  '''\n",
        "  for layer in m.children():\n",
        "      if hasattr(layer, 'reset_parameters'):\n",
        "          print(f'Reset trainable parameters of layer = {layer}')\n",
        "          layer.reset_parameters()\n",
        "\n",
        "def try_gpu():\n",
        "    \"\"\"\n",
        "    If GPU is available, return torch.device as cuda:0; else return torch.device\n",
        "    as cpu.\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda:0')\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "    return device\n",
        "    \n",
        "class Net_d(nn.Module):\n",
        "    \"\"\"\n",
        "    3-layer CNN network with max pooling\n",
        "    \n",
        "    Args:\n",
        "        in_channels: number of features of the input image (\"depth of image\")\n",
        "        hidden_channels: number of hidden features (\"depth of convolved images\")\n",
        "        out_features: number of features in output layer\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, in_features, hidden_dim, out_features):\n",
        "        super(Net_d, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Linear(in_features, hidden_dim[0])\n",
        "        self.relu1 = nn.ReLU()\n",
        "        \n",
        "        self.layer2 = nn.Linear(hidden_dim[0], hidden_dim[1])\n",
        "        self.relu2 = nn.ReLU()\n",
        "        \n",
        "        self.layer3 = nn.Linear(hidden_dim[1], hidden_dim[2])\n",
        "        self.relu3 = nn.ReLU()\n",
        "        \n",
        "        self.layer4 = nn.Linear(hidden_dim[2], hidden_dim[3])\n",
        "        self.relu4 = nn.ReLU()\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim[3], out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.fc(x)\n",
        "        \n",
        "        return x   \n",
        "        \n",
        "def custom_loss_d(y_pred, y_batch):\n",
        "    \n",
        "    loss = torch.mean(torch.abs(y_batch - y_pred))*128*2\n",
        "    \n",
        "    constraint = torch.sum(torch.abs(-4 - torch.sum(y_pred, axis = 1)))/(y_batch.shape[0])\n",
        "    \n",
        "    return (loss + constraint)\n",
        "\n",
        "def custom_loss_d1(y_pred, y_batch):\n",
        "    \n",
        "    loss = torch.mean(torch.abs(y_batch - y_pred))\n",
        "    \n",
        "    constraint = (1/128)*torch.sum(torch.abs(-4 - torch.sum(y_pred, axis = 1)))/(y_batch.shape[0])\n",
        "    \n",
        "    return (loss + constraint)\n",
        "\n",
        "    \n",
        "def custom_loss_2(y_pred, y_batch):\n",
        "    \n",
        "    loss = torch.mean(torch.abs(y_batch - y_pred))*128\n",
        "    \n",
        "    constraint = torch.sum(torch.abs(-4 - torch.sum(y_pred, axis = 1)))/(y_batch.shape[0])/64\n",
        "    \n",
        "    return (loss + constraint)\n",
        "    \n",
        "\n",
        "def custom_loss_3(y_pred, y_batch):\n",
        "    \n",
        "    loss = torch.square(torch.mean(torch.abs(y_batch - y_pred))*128)\n",
        "    \n",
        "    constraint = torch.square(torch.sum(torch.abs(-4 - torch.sum(y_pred, axis = 1)))/(y_batch.shape[0]))\n",
        "    \n",
        "    return (loss + constraint)\n",
        "    \n",
        "def custom_loss_4(y_pred, y_batch):\n",
        "    \n",
        "    loss = torch.mean(torch.abs(y_batch - y_pred))*256\n",
        "    \n",
        "    constraint = torch.sum(torch.abs(-4 - torch.sum(y_pred, axis = 1)))/(y_batch.shape[0])\n",
        "    \n",
        "    return (loss + constraint)\n",
        "\n",
        "def evaluate_accuracy(data_loader, net, device):\n",
        "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
        "    net.to(device)\n",
        "    net.eval()  #make sure network is in evaluation mode\n",
        "\n",
        "    #init\n",
        "    loss_sum = torch.tensor([0], dtype=torch.float32, device=device)\n",
        "    con_sum = torch.tensor([0], dtype=torch.float32, device=device)\n",
        "    n = 0\n",
        "\n",
        "    for X, y in data_loader:\n",
        "        # Copy the data to device.\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "         \n",
        "            loss_sum += torch.sum(torch.abs(net(X) - y))/128\n",
        "            con_sum += torch.sum(torch.abs(torch.sum(net(X), axis = 1) - torch.sum(y,axis = 1)))/128\n",
        "            \n",
        "            n += y.shape[0] #increases with the number of samples in the batch\n",
        "    return loss_sum.item()/n, con_sum.item()/n\n",
        "\n",
        "def train_test(epochs, net, device, train_loader, test_loader, optimizer, criterion):\n",
        "    \"\"\"Training and testing function of FCN\"\"\"\n",
        "    \n",
        "    startTime = datetime.now()\n",
        "    \n",
        "    # Define list to store losses and performances of each iteration\n",
        "    train_losses = []\n",
        "    train_losss = []\n",
        "    test_losss = []\n",
        "    train_cons = []\n",
        "    test_cons = []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # Network in training mode and to device\n",
        "        net.train()\n",
        "        net.to(device)\n",
        "        \n",
        "        # Training loop\n",
        "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
        "\n",
        "            # Set to same device\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            # Set the gradients to zero\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Perform forward pass\n",
        "            y_pred = net(x_batch)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(y_pred, y_batch)\n",
        "            train_losses.append(loss.detach().cpu().numpy())\n",
        "            \n",
        "            # Backward computation and update\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(datetime.now() - startTime)\n",
        "        \n",
        "        if epoch == 25: \n",
        "            learning_rate = 0.0001\n",
        "            optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0.0)\n",
        "        elif epoch == 50:\n",
        "            learning_rate = 0.00001\n",
        "            optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0.0)\n",
        "        elif epoch == 75:\n",
        "            learning_rate = 0.00001\n",
        "            optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0.0) \n",
        "            \n",
        "        if  epoch == 0 or epoch%5 == 0:\n",
        "            # Compute train and test error\n",
        "            train_loss, train_con = evaluate_accuracy(train_loader, net, device)\n",
        "            test_loss, test_con = evaluate_accuracy(test_loader, net, device)\n",
        "            \n",
        "            # Development of performance\n",
        "            train_losss.append(train_loss)\n",
        "            test_losss.append(test_loss)\n",
        "            \n",
        "            train_cons.append(train_con)\n",
        "            test_cons.append(test_con)\n",
        "        \n",
        "            # Print performance\n",
        "            print('Epoch: {:.0f}'.format(epoch+1))\n",
        "            print('Loss on train set: {:.7f}'.format(train_loss))\n",
        "            print('Loss on test set: {:.7f}'.format(test_loss))\n",
        "            print('Con on train set: {:.7f}'.format(train_con))\n",
        "            print('Con on test set: {:.7f}'.format(test_con))\n",
        "            print('')\n",
        "\n",
        "    print(datetime.now() - startTime)\n",
        "    \n",
        "    return net, train_losses, train_losss, test_losss, train_cons, test_cons\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWABQmmt0XrZ"
      },
      "outputs": [],
      "source": [
        "in_features = 222\n",
        "in_features_qxyz = 296\n",
        "in_features_H2O = 672\n",
        "out_features = 128\n",
        "\n",
        "# default\n",
        "hidden_dim_d = [1024, 1024, 1024, 1024]\n",
        "\n",
        "net_d = Net_d(in_features, hidden_dim_d, out_features)\n",
        "\n",
        "# neurons\n",
        "hidden_dim_2 = [1024, 512, 256, 128]\n",
        "hidden_dim_3 = [512, 512, 512, 512]\n",
        "hidden_dim_4 = [1524, 1524, 1524, 1524]\n",
        "\n",
        "net_2 = Net_d(in_features, hidden_dim_2, out_features)\n",
        "net_3 = Net_d(in_features, hidden_dim_3, out_features)\n",
        "net_4 = Net_d(in_features, hidden_dim_4, out_features)\n",
        "\n",
        "# layers\n",
        "hidden_dim_5 = [1024, 1024, 1024]\n",
        "hidden_dim_6 = [1024, 1024, 1024, 1024, 1024]\n",
        "\n",
        "net_5 = Net_2(in_features, hidden_dim_5, out_features)\n",
        "net_6 = Net_3(in_features, hidden_dim_6, out_features)\n",
        "\n",
        "# activation function\n",
        "net_7 = Net_act_1(in_features, hidden_dim_d, out_features)\n",
        "\n",
        "# qxyz\n",
        "net_9 = Net_d(in_features_qxyz, hidden_dim_d, out_features)\n",
        "# H2O\n",
        "net_10 = Net_d(in_features_H2O, hidden_dim_d, out_features)\n",
        "\n",
        "epochs = 101\n",
        "\n",
        "# optimizer\n",
        "learning_rate_d = 0.001\n",
        "optimizer_d = torch.optim.Adam(net_d.parameters(), lr=learning_rate_d, weight_decay=0.5)\n",
        "\n",
        "hidden_dim_11 = [750, 750, 750, 1024, 1024]\n",
        "net_11 = Net_10(in_features, hidden_dim_11, out_features)\n",
        "\n",
        "# loss function\n",
        "criterion_d = custom_loss_d\n",
        "criterion_d1 = custom_loss_d1\n",
        "criterion_2 = custom_loss_2\n",
        "criterion_3 = custom_loss_3\n",
        "criterion_4 = custom_loss_4\n",
        "\n",
        "device = try_gpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ywb8Mb3J0ZSP",
        "outputId": "56102f95-224f-439a-b497-9620a8346a75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reset trainable parameters of layer = Linear(in_features=672, out_features=1024, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=1024, out_features=1024, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=1024, out_features=1024, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=1024, out_features=1024, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=1024, out_features=128, bias=True)\n",
            "0:03:45.616269\n",
            "Epoch: 1\n",
            "Loss on train set: 0.0338394\n",
            "Loss on test set: 0.0399452\n",
            "Con on train set: 0.0010451\n",
            "Con on test set: 0.0007874\n",
            "\n",
            "0:07:27.281884\n",
            "0:11:03.461363\n",
            "0:14:29.691009\n",
            "0:17:48.049084\n",
            "0:21:07.882130\n",
            "Epoch: 6\n",
            "Loss on train set: 0.0321993\n",
            "Loss on test set: 0.0402334\n",
            "Con on train set: 0.0010559\n",
            "Con on test set: 0.0015022\n",
            "\n",
            "0:24:25.457133\n",
            "0:27:41.166943\n",
            "0:30:57.017458\n",
            "0:34:16.362605\n",
            "0:37:33.679601\n",
            "Epoch: 11\n",
            "Loss on train set: 0.0318426\n",
            "Loss on test set: 0.0397516\n",
            "Con on train set: 0.0011162\n",
            "Con on test set: 0.0009673\n",
            "\n",
            "0:40:51.943708\n",
            "0:44:09.932172\n",
            "0:47:27.481271\n",
            "0:50:44.543153\n",
            "0:54:01.533225\n",
            "Epoch: 16\n",
            "Loss on train set: 0.0318273\n",
            "Loss on test set: 0.0400368\n",
            "Con on train set: 0.0010671\n",
            "Con on test set: 0.0013046\n",
            "\n",
            "0:57:19.002474\n",
            "1:00:35.141508\n",
            "1:03:50.782279\n",
            "1:07:06.516409\n",
            "1:10:21.634868\n",
            "Epoch: 21\n",
            "Loss on train set: 0.0315724\n",
            "Loss on test set: 0.0403027\n",
            "Con on train set: 0.0011475\n",
            "Con on test set: 0.0010964\n",
            "\n",
            "1:13:37.357885\n",
            "1:16:51.454395\n",
            "1:20:06.011598\n",
            "1:23:19.971524\n",
            "1:26:31.340053\n",
            "Epoch: 26\n",
            "Loss on train set: 0.0315570\n",
            "Loss on test set: 0.0396057\n",
            "Con on train set: 0.0013283\n",
            "Con on test set: 0.0016530\n",
            "\n",
            "1:29:43.490101\n",
            "1:32:54.246424\n",
            "1:36:05.388787\n",
            "1:39:16.800015\n",
            "1:42:28.193984\n",
            "Epoch: 31\n",
            "Loss on train set: 0.0300073\n",
            "Loss on test set: 0.0394700\n",
            "Con on train set: 0.0012962\n",
            "Con on test set: 0.0011192\n",
            "\n",
            "1:45:40.498336\n",
            "1:48:52.452725\n",
            "1:52:03.740334\n",
            "1:55:16.221413\n",
            "1:58:28.978128\n",
            "Epoch: 36\n",
            "Loss on train set: 0.0295815\n",
            "Loss on test set: 0.0396899\n",
            "Con on train set: 0.0013173\n",
            "Con on test set: 0.0012556\n",
            "\n",
            "2:01:42.805563\n",
            "2:04:54.776232\n",
            "2:08:06.832937\n",
            "2:11:19.331971\n",
            "2:14:31.934205\n",
            "Epoch: 41\n",
            "Loss on train set: 0.0293204\n",
            "Loss on test set: 0.0402417\n",
            "Con on train set: 0.0012730\n",
            "Con on test set: 0.0012013\n",
            "\n",
            "2:17:45.742627\n",
            "2:20:58.165900\n",
            "2:24:10.831661\n",
            "2:27:23.086402\n",
            "2:30:35.250877\n",
            "Epoch: 46\n",
            "Loss on train set: 0.0291733\n",
            "Loss on test set: 0.0400998\n",
            "Con on train set: 0.0012555\n",
            "Con on test set: 0.0011967\n",
            "\n",
            "2:33:48.368097\n",
            "2:37:02.731678\n",
            "2:40:18.557264\n",
            "2:43:34.511079\n",
            "2:46:47.959309\n",
            "Epoch: 51\n",
            "Loss on train set: 0.0290340\n",
            "Loss on test set: 0.0403481\n",
            "Con on train set: 0.0012423\n",
            "Con on test set: 0.0011977\n",
            "\n",
            "2:50:05.319592\n",
            "2:53:20.133917\n",
            "2:56:33.501453\n",
            "2:59:45.630627\n",
            "3:03:03.102591\n",
            "Epoch: 56\n",
            "Loss on train set: 0.0288765\n",
            "Loss on test set: 0.0401935\n",
            "Con on train set: 0.0012643\n",
            "Con on test set: 0.0011755\n",
            "\n",
            "3:06:19.730609\n",
            "3:09:32.310021\n",
            "3:12:44.712142\n",
            "3:15:56.703504\n",
            "3:19:09.662811\n",
            "Epoch: 61\n",
            "Loss on train set: 0.0288532\n",
            "Loss on test set: 0.0403222\n",
            "Con on train set: 0.0012788\n",
            "Con on test set: 0.0012028\n",
            "\n",
            "3:22:23.000146\n",
            "3:25:35.747414\n",
            "3:28:48.776349\n",
            "3:32:01.873599\n",
            "3:35:14.778644\n",
            "Epoch: 66\n",
            "Loss on train set: 0.0288378\n",
            "Loss on test set: 0.0402840\n",
            "Con on train set: 0.0012536\n",
            "Con on test set: 0.0011706\n",
            "\n",
            "3:38:28.708888\n",
            "3:41:41.664428\n",
            "3:44:55.613219\n",
            "3:48:14.130078\n",
            "3:51:32.095818\n",
            "Epoch: 71\n",
            "Loss on train set: 0.0288225\n",
            "Loss on test set: 0.0404291\n",
            "Con on train set: 0.0012620\n",
            "Con on test set: 0.0012005\n",
            "\n",
            "3:54:49.829488\n",
            "3:58:05.449496\n",
            "4:01:19.040085\n",
            "4:04:32.505217\n",
            "4:07:45.740167\n",
            "Epoch: 76\n",
            "Loss on train set: 0.0288098\n",
            "Loss on test set: 0.0402212\n",
            "Con on train set: 0.0012593\n",
            "Con on test set: 0.0011545\n",
            "\n",
            "4:11:02.000659\n",
            "4:14:16.911128\n",
            "4:17:30.300728\n",
            "4:20:44.073577\n",
            "4:23:57.761963\n",
            "Epoch: 81\n",
            "Loss on train set: 0.0288012\n",
            "Loss on test set: 0.0402183\n",
            "Con on train set: 0.0012596\n",
            "Con on test set: 0.0011567\n",
            "\n",
            "4:27:12.636650\n",
            "4:30:26.087344\n",
            "4:33:40.146940\n",
            "4:36:56.058069\n",
            "4:40:09.835336\n",
            "Epoch: 86\n",
            "Loss on train set: 0.0287994\n",
            "Loss on test set: 0.0402348\n",
            "Con on train set: 0.0012589\n",
            "Con on test set: 0.0011576\n",
            "\n",
            "4:43:25.191381\n",
            "4:46:39.049894\n",
            "4:49:53.614021\n",
            "4:53:07.479445\n",
            "4:56:21.905554\n",
            "Epoch: 91\n",
            "Loss on train set: 0.0287977\n",
            "Loss on test set: 0.0402415\n",
            "Con on train set: 0.0012593\n",
            "Con on test set: 0.0011578\n",
            "\n",
            "4:59:37.349795\n",
            "5:02:51.391979\n",
            "5:06:04.835532\n",
            "5:09:18.952336\n",
            "5:12:33.380127\n",
            "Epoch: 96\n",
            "Loss on train set: 0.0287964\n",
            "Loss on test set: 0.0402307\n",
            "Con on train set: 0.0012579\n",
            "Con on test set: 0.0011539\n",
            "\n",
            "5:15:48.669431\n",
            "5:19:02.619686\n",
            "5:22:16.363261\n",
            "5:25:34.206348\n",
            "5:25:34.206722\n"
          ]
        }
      ],
      "source": [
        "train_loader_H2O = torch.load('train_loader_XH2O_3D')\n",
        "test_loader_H2O = torch.load('test_loader_XH2O_3D')\n",
        "\n",
        "net_10.apply(reset_weights)\n",
        "optimizer_d = torch.optim.Adam(net_10.parameters(), lr=learning_rate_d, weight_decay=0.5)\n",
        "net_H2O, train_losses_H2O, train_losss_H2O, test_losss_H2O, train_cons_H2O, test_cons_H2O = train_test(epochs, net_10, device, train_loader_H2O, test_loader_H2O, optimizer_d, criterion_d)\n",
        "torch.save(net_H2O,'net_H2O_2')\n",
        "np.savez('net_H2O_2.npz', trainlossH2O=train_losses_H2O, trainlosssH2O=train_losss_H2O, testlosssH2O=test_losss_H2O, trainconH2O=train_cons_H2O, testconH2O=test_cons_H2O)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "name": "FCN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}